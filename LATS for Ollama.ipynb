{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LATS for Ollama\n",
    "\n",
    "Here is my primitive implementation of LATS using basic Langchain functionalities alongside with a simpler structure. I built this LATS structure to be as modifiable as possible, with different prompt schemas and value functions available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_if_undefined(var: str) -> None:\n",
    "    if os.environ.get(var):\n",
    "        return\n",
    "    os.environ[var] = getpass.getpass(var)\n",
    "#Configure Tavily API key here\n",
    "_set_if_undefined(\"TAVILY_API_KEY\")\n",
    "#You won't need this if you won't use OpenAI LLMs\n",
    "_set_if_undefined(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "Here is the tree itself. Much of the code was taken / inspired from https://github.com/langchain-ai/langgraph/blob/main/examples/lats/lats.ipynb\n",
    "\n",
    "Also check out this article: https://medium.com/pythoneers/power-up-ollama-chatbots-with-tools-113ed8229a7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage, ToolMessage\n",
    "from collections import deque\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, \n",
    "                 messages: list[BaseMessage],\n",
    "                 reflection,\n",
    "                 parent: Optional[Node] = None,\n",
    "                 exploration_weight: Optional[float] = 1.0,\n",
    "                 ):\n",
    "        self.messages = messages\n",
    "        self.reflection = reflection\n",
    "        self.parent = parent\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.value = 0\n",
    "        self.exploration_weight = exploration_weight\n",
    "        self.depth = parent.depth + 1 if parent is not None else 1\n",
    "        self._is_solved = reflection.found_solution if reflection else False\n",
    "        self.backpropagate(reflection.normalized_score)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            f\"<Node average value={self.avg_value}, visits={self.visits},\"\n",
    "            f\" solution={self.messages} reflection={self.reflection}/>\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def avg_value(self):\n",
    "        \"\"\"Average value. Stands for win/total visits ratio in a normal MCTS\"\"\"\n",
    "        return self.value if self.visits == 0 else self.visits/self.value\n",
    "\n",
    "    @property\n",
    "    def upper_confidence_bound(self):\n",
    "        \"\"\"The most integral part of MCTS. Balances exploration by choosing nodes with high value and low exploration depending on the exploration_weight\"\"\"\n",
    "        if self.parent is None:\n",
    "            #raise ValueError(\"Cannot obtain UCT from root node\")\n",
    "            return 0\n",
    "        if self.visits == 0:\n",
    "            return self.value\n",
    "        exploration_term = math.sqrt(math.log(self.parent.visits) / self.visits)\n",
    "        return self.avg_value + self.exploration_weight * exploration_term\n",
    "    \n",
    "    @property\n",
    "    def is_tree_solved(self):\n",
    "        \"\"\"Determines if the tree is solved.\"\"\"\n",
    "        return any(node._is_solved for node in self.all_children)\n",
    "    \n",
    "    @property\n",
    "    def solved_node(self):\n",
    "        \"\"\"Returns the node who solved the prompt.\"\"\"\n",
    "        return next((node for node in self.all_children if node._is_solved), None)\n",
    "    \n",
    "    def backpropagate(self, value: float):\n",
    "        node = self\n",
    "        while node:\n",
    "            node.visits += 1\n",
    "            node.value += value\n",
    "            node = node.parent\n",
    "\n",
    "    @property\n",
    "    def all_children(self):\n",
    "        #taken directly\n",
    "        all_nodes = []\n",
    "        nodes = deque()\n",
    "        nodes.append(self)\n",
    "        all_nodes.append(self)\n",
    "        while nodes:\n",
    "            node = nodes.popleft()\n",
    "            all_nodes.extend(node.children)\n",
    "            for n in node.children:\n",
    "                nodes.append(n)\n",
    "        return all_nodes\n",
    "\n",
    "    @property\n",
    "    def all_messages(self):\n",
    "        node = self\n",
    "        message_list = []\n",
    "        while node.depth > 1:            \n",
    "            message_list.extend([node.reflection.as_message(), node.messages])\n",
    "            node = node.parent\n",
    "        return message_list[::-1]\n",
    "    \n",
    "    @property \n",
    "    def best_child(self):\n",
    "        \"\"\"Gets the best children for the next step of MCTS iteration\"\"\"\n",
    "        #A heap could be used instead of searching the whole structure all the time\n",
    "        return max(self.all_children, key=lambda child: child.upper_confidence_bound)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM & Tools\n",
    "We define the LLM model and the tools in use (Tavily for this example) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_community.utilities.tavily_search import TavilySearchAPIWrapper\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = Ollama(model=\"llama3\", format=\"json\") #ChatOpenAI(model=\"gpt-4o\") \n",
    "tool_llm = llm\n",
    "reasoning_llm = llm\n",
    "reflection_llm = llm\n",
    "search = TavilySearchAPIWrapper()\n",
    "tavily_tool = TavilySearchResults(api_wrapper=search, max_results=5)\n",
    "tools = [tavily_tool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Templates & Prompt Models\n",
    "To make LATS as accessible as possible, we avoid using API specific methods. This means we have to implement pydantic parsing, tool calling ourselves. \n",
    "We will have 3 chains to simulate LATS alongside with tool calls.\n",
    "- Tool Chain: Here we will define our tavily tool (or more!) and get the query.\n",
    "- Reasoning Chain: We feed the tool output into the LLM to answer the prompt itself.\n",
    "- Reflection Chain: The LLM rates the prompt quality in the end and the node gets created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.render import render_text_description \n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from operator import itemgetter\n",
    "import json\n",
    "\n",
    "class ToolCall(BaseModel):\n",
    "    tool_name: str = Field(\n",
    "        description=\"The name of the tool being called\"\n",
    "    )\n",
    "    args: dict[str,str] = Field(\n",
    "        description=\"Arguments for the tool being called in the form of a dictionary\"\n",
    "    )\n",
    "\n",
    "#taken directly\n",
    "class Reflection(BaseModel):\n",
    "    reflections: str = Field(\n",
    "        description=\"The critique and reflections on the sufficiency, superfluency,\"\n",
    "        \" and general quality of the response\"\n",
    "    )\n",
    "    score: int = Field(\n",
    "        description=\"Score from 0-10 on the quality of the candidate response.\",\n",
    "        gte=0,\n",
    "        lte=10,\n",
    "    )\n",
    "    found_solution: bool = Field(\n",
    "        description=\"Whether the response has fully solved the question or task.\"\n",
    "    )\n",
    "\n",
    "    def as_message(self):\n",
    "        return HumanMessage(\n",
    "            content=f\"Reasoning: {self.reflections}\\nScore: {self.score}\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def normalized_score(self) -> float:\n",
    "        return self.score / 10.0\n",
    "\n",
    "tool_parser = PydanticOutputParser(pydantic_object=ToolCall)\n",
    "tool_format_instructions = tool_parser.get_format_instructions()\n",
    "tool_descriptions = render_text_description(tools)\n",
    "tool_system_prompt = \"\"\"You are an assistant that has access to the following set of tools.\n",
    "Here are the names and descriptions for each tool:\n",
    "{tool_descriptions}\n",
    "You must specify the tool name and give the method input variables into args as follows:\n",
    "{tool_format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "tool_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            tool_system_prompt,\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "def tool_runner(model_output : ToolCall):\n",
    "    tool_map = {tool.name: tool for tool in tools}\n",
    "    chosen_tool = tool_map[model_output.tool_name]\n",
    "    return chosen_tool(model_output.args)\n",
    "\n",
    "tool_chain = tool_prompt | tool_llm | tool_parser | tool_runner\n",
    "#tool_chain.invoke({input : \"search me about lithium\"})\n",
    "#temp = tool_chain.invoke({\n",
    "#    \"input\": \"search me about lithium\", \n",
    "#    \"tool_format_instructions\": tool_format_instructions, \n",
    "#    \"tool_descriptions\": tool_descriptions\n",
    "#})\n",
    "\n",
    "reasoning_system_prompt = \"\"\"\n",
    "Think step by step. You are an AI assistant that will answer the given question as a detailed paragraph on the given tool info, your previous thoughts and reflections.\n",
    "\"\"\"\n",
    "reasoning_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            reasoning_system_prompt,\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "reasoning_chain = reasoning_prompt | reasoning_llm\n",
    "reflection_parser = PydanticOutputParser(pydantic_object=Reflection)\n",
    "reflection_format_instructions = reflection_parser.get_format_instructions()\n",
    "reflection_system_prompt = \"\"\"\n",
    "Criticize the given line of thoughts and score it.\n",
    "You must answer in this format as a json:\n",
    "{reflection_format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            reflection_system_prompt,\n",
    "        ),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
    "    ]\n",
    ")\n",
    "reflection_chain = reflection_prompt | reflection_llm | reflection_parser\n",
    "#usage: reflection_chain.invoke({\"input\": \"user input\", \"reflection_format_instructions\": \"put the pydantic format here\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree States\n",
    "We will handle the tree states here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LATS:\n",
    "    def __init__(self,\n",
    "                 user_prompt: str,\n",
    "                 max_depth: Optional[int] = 5,\n",
    "                 leaf_number: Optional[int] = 5,\n",
    "                 exploration_weight: Optional[float] = 1.0,\n",
    "                 max_tries: Optional[int] = 3\n",
    "                 ):\n",
    "            self.user_prompt = user_prompt\n",
    "            self.max_depth = max_depth\n",
    "            self.leaf_number = leaf_number\n",
    "            self.max_tries = max_tries\n",
    "            self.exploration_weight = exploration_weight\n",
    "            init_messages = [HumanMessage(content = user_prompt),]\n",
    "            init_reflection = Reflection(reflections = \"\", score = 0, found_solution = False) #Dummy values for root node\n",
    "            #In theory, these dummy values shouldn't have any effect even though we start with 1 visit in root.\n",
    "            self.root_node = Node(init_messages, init_reflection, exploration_weight = self.exploration_weight)\n",
    "\n",
    "    def invoke(self):\n",
    "        \"\"\"Runs the tree.\"\"\"\n",
    "        ended = False\n",
    "        iteration = 1\n",
    "        while not ended:\n",
    "          print(\"Iteration \", iteration)\n",
    "          status = self.expand()\n",
    "          iteration += 1\n",
    "          if(status != 'CONTINUE'):\n",
    "            ended = True\n",
    "        self.print_solution()\n",
    "\n",
    "    def expand(self):\n",
    "          \"\"\"Expands the best child.\"\"\"\n",
    "          node = self.root_node.best_child\n",
    "          if(node.depth >= self.max_depth):\n",
    "                return \"END\"\n",
    "          #add parallelization here\n",
    "          for i in range(self.leaf_number):\n",
    "            print(\"Node iteration:\", i + 1)\n",
    "            messages = node.all_messages\n",
    "            tries = 0\n",
    "            new_node_messages = []  # messages for the node we will initialize\n",
    "\n",
    "            # Attempt to invoke tool_chain\n",
    "            while tries < self.max_tries:\n",
    "                try:\n",
    "                    tool_message = tool_chain.invoke({\n",
    "                        \"input\": self.user_prompt, \n",
    "                        \"tool_format_instructions\": tool_format_instructions, \n",
    "                        \"tool_descriptions\": tool_descriptions,\n",
    "                        \"messages\": messages,\n",
    "                    })\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    tries += 1  # Increment tries\n",
    "            else:\n",
    "                print(\"Max tries exceeded for tool_chain\")\n",
    "                return \"ENDED\"\n",
    "\n",
    "            tool_message = ToolMessage(content=tool_message, tool_call_id=0)  # call id not important for us\n",
    "            messages.append(tool_message)\n",
    "\n",
    "            # Reset tries for reasoning_chain\n",
    "            tries = 0\n",
    "\n",
    "            # Attempt to invoke reasoning_chain\n",
    "            while tries < self.max_tries:\n",
    "                try:\n",
    "                    reasoning_message = reasoning_chain.invoke({\n",
    "                        \"input\": self.user_prompt,\n",
    "                        \"messages\": messages\n",
    "                    })\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    tries += 1  # Increment tries\n",
    "            else:\n",
    "                print(\"Max tries exceeded for reasoning_chain\")\n",
    "                return \"ENDED\"\n",
    "\n",
    "            reasoning_message = AIMessage(content=reasoning_message)\n",
    "            messages.append(reasoning_message)\n",
    "\n",
    "            # Reset tries for reflection_chain\n",
    "            tries = 0\n",
    "\n",
    "            # Attempt to invoke reflection_chain\n",
    "            while tries < self.max_tries:\n",
    "                try:\n",
    "                    reflection_message = reflection_chain.invoke({\n",
    "                        \"input\": self.user_prompt,\n",
    "                        \"reflection_format_instructions\": reflection_format_instructions,\n",
    "                        \"messages\": messages    \n",
    "                    })\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    tries += 1  # Increment tries\n",
    "            else:\n",
    "                print(\"Max tries exceeded for reflection_chain\")\n",
    "                return \"ENDED\"\n",
    "\n",
    "            new_node_messages = [tool_message, reasoning_message]\n",
    "            new_node = Node(new_node_messages, reflection_message, self.root_node, exploration_weight=self.exploration_weight)\n",
    "            node.children.append(new_node)\n",
    "\n",
    "            if reflection_message.found_solution:\n",
    "                return \"SOLVED\"\n",
    "          return \"CONTINUE\"\n",
    "\n",
    "\n",
    "    def print_solution(self):\n",
    "      solution_node = self.root_node.solved_node\n",
    "      if solution_node is None:\n",
    "        print(\"No solution has been found in this tree.\")\n",
    "      else:\n",
    "        print(solution_node.all_messages)\n",
    "      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using The Tree\n",
    "Congratulations! You have implemented the tree! Using it is relatively simple, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  1\n",
      "Node iteration: 1\n",
      "[[ToolMessage(content=[{'url': 'https://medium.com/@mauryaanoop3/unleashing-structured-responses-functional-calling-with-langchain-ollama-and-phi-3-part-3-720b34203778', 'content': \"In the previous articles, we explored functional calling with LangChain, Ollama, and Microsoft's Phi-3 model. We focused on functional calling, demonstrating how to interact with the LLM and ...\"}, {'url': 'https://blog.langchain.dev/json-based-agents-with-ollama-and-langchain/', 'content': \"When the LLM needs to call a function, it should use the following JSON structure:\\nThatâ€™s why it is called a JSON-based agent: we instruct the LLM to produce a JSON when it wants to use any available tools. Since one of the available tools of the agent is a recommender tool, it decided to utilize the recommender tool by providing the JSON syntax to define its input. Since the tool provided all the required information, the LLM decided that it had enough information to construct a final answer, which could be returned to the user.\\n JSON agents with Ollama & LangChain\\nLearn to implement an open-source Mixtral agent that interacts with a graph database Neo4j through a semantic layer\\nEditor's note: This post is written by Tomaz Bratanic from Neo4j\\nBy now, we all have probably recognized that we can significantly enhance the capabilities of LLMs by providing them with additional tools. The full output should have the following structure:\\nThe LLM should always expain what is it doing in the thought part of the output.\"}, {'url': 'https://js.langchain.com/v0.1/docs/integrations/chat/ollama_functions/', 'content': 'Ollama Functions. LangChain offers an experimental wrapper around open source models run locally via Ollama that gives it the same API as OpenAI Functions. Note that more powerful and capable models will perform better with complex schema and/or multiple functions. The examples below use Mistral.'}, {'url': 'https://python.langchain.com/v0.1/docs/integrations/llms/ollama/', 'content': 'Ollama allows you to run open-source large language models, such as Llama 2, locally. Ollama bundles model weights, configuration, and data into a single package, defined by a Modelfile. It optimizes setup and configuration details, including GPU usage. For a complete list of supported models and model variants, see the Ollama model library.'}, {'url': 'https://medium.com/@mauryaanoop3/unleashing-llms-functional-calling-with-langchain-ollama-and-microsofts-phi-3-part-2-10fae91d7b01', 'content': 'model = OllamaFunctions(model=\"phi3\", keep_alive=-1, format=\"json\") ... This example demonstrates a basic functional call using LangChain, Ollama, and Phi-3. With this approach, you can explore ...'}], tool_call_id='0'), AIMessage(content='{\\n  \"response\": {\\n    \"source\": {\\n      \"text\": [\\n        \"OllamaFunctions in Langchain refer to an experimental wrapper around open source models that are run locally via Ollama and provide the same API as OpenAI Functions. In this context, \\'functional calling\\' implies utilizing these functions programmatically to interact with language models like Microsoft\\'s Phi-3.\",\\n        \"LangChain allows you to call specific functions by providing a JSON structure that outlines your requirements or queries. This is also known as using a JSON-based agent. It provides an efficient way for LLM (Large Language Models) to communicate with various tools and services in the ecosystem.\",\\n        \"Ollama allows you to run open-source large language models, such as Llama 2, locally by bundling model weights, configuration, and data into a single package defined by a Modelfile. Ollama optimizes setup and configuration details, including GPU usage for efficient local execution of the models.\",\\n        \"In LangChain, you can use OllamaFunctions to interact with Phi-3 by creating an instance of OllamaFunctions, specifying your model (Phi-3 in this case), keeping the LLM alive (\\'keep_alive\\' set to -1 for infinite keep alive) and defining output format as JSON.\",\\n        \"To use these functions effectively, you need to provide a well-structured JSON representation of your requirements or queries. This JSON structure acts as an instruction that tells LangChain what action the LLM should perform on behayer data provided by various tools in the ecosystem.\"\\n      ]\\n    },\\n    \"understanding\": [\\n      {\\n        \"explanation\": \"OllamaFunctions serve as a bridge between LangChain and open-source language models, allowing developers to leverage these powerful models within their applications.\",\\n        \"action_step\": \"Begin by creating an instance of OllamaFunctions with your preferred model. You can then define how the LLM should process incoming data in the \\'thought\\' part of its output.\"\\n      },\\n      {\\n        \"explanation\": \"The use of JSON-based agents provides a standardized way to interact with language models, ensuring that all necessary information is conveyed and understood by the model.\",\\n        \"action_step\": \"Always construct your JSON structure clearly and comprehensively to ensure accurate communication between LangChain and the LLM.\"\\n      },\\n      {\\n        \"explanation\": \"By running open-source language models locally with Ollama, developers gain flexibility in using various models while optimizing performance through local execution.\",\\n        \"action_step\": \"Choose a model from the supported list provided by Ollama and configure it according to your project\\'s requirements.\"\\n      }\\n    ],\\n    \"previous_thoughts\": [\\n      {\\n        \"reflection\": \"It is crucial to understand how LangChain, OllamaFunctions, and Phi-3 interplay in order to effectively use these tools. The combination of functional calling, JSON-based agents, and local model execution through Ollama presents a robust framework for leveraging language models within applications.\"\\n      }\\n    ]\\n  }\\n}')], HumanMessage(content=\"Reasoning: The given line discusses the key components and usage patterns of LangChain's OllamaFunctions, emphasizing their role in enabling developers to interact with open-source LLM models like Phi-3. It underscores the importance of structured JSON input for functional calling and the benefits of local execution facilitated by Ollama.\\nScore: 9\")]\n"
     ]
    }
   ],
   "source": [
    "question = \"Give me a summary of what OllamaFunctions are in Langchain and how can I use them?\"\n",
    "tree = LATS(user_prompt = question)\n",
    "tree.invoke()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
